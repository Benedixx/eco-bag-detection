{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "if os.getcwd() == 'd:\\\\TITO\\\\Documents\\\\Deep-learning\\\\eco-bag-detection\\\\models\\\\research':\n",
    "    print('Already in directory')\n",
    "else:\n",
    "    os.chdir('models/research')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(tf) d:\\TITO\\Documents\\Deep-learning\\eco-bag-detection\\models\\research>protoc object_detection\\protos\\anchor_generator.proto --python_out=. \n",
      "\n",
      "(tf) d:\\TITO\\Documents\\Deep-learning\\eco-bag-detection\\models\\research>protoc object_detection\\protos\\argmax_matcher.proto --python_out=. \n",
      "\n",
      "(tf) d:\\TITO\\Documents\\Deep-learning\\eco-bag-detection\\models\\research>protoc object_detection\\protos\\bipartite_matcher.proto --python_out=. \n",
      "\n",
      "(tf) d:\\TITO\\Documents\\Deep-learning\\eco-bag-detection\\models\\research>protoc object_detection\\protos\\box_coder.proto --python_out=. \n",
      "\n",
      "(tf) d:\\TITO\\Documents\\Deep-learning\\eco-bag-detection\\models\\research>protoc object_detection\\protos\\box_predictor.proto --python_out=. \n",
      "\n",
      "(tf) d:\\TITO\\Documents\\Deep-learning\\eco-bag-detection\\models\\research>protoc object_detection\\protos\\calibration.proto --python_out=. \n",
      "\n",
      "(tf) d:\\TITO\\Documents\\Deep-learning\\eco-bag-detection\\models\\research>protoc object_detection\\protos\\center_net.proto --python_out=. \n",
      "\n",
      "(tf) d:\\TITO\\Documents\\Deep-learning\\eco-bag-detection\\models\\research>protoc object_detection\\protos\\eval.proto --python_out=. \n",
      "\n",
      "(tf) d:\\TITO\\Documents\\Deep-learning\\eco-bag-detection\\models\\research>protoc object_detection\\protos\\faster_rcnn.proto --python_out=. \n",
      "\n",
      "(tf) d:\\TITO\\Documents\\Deep-learning\\eco-bag-detection\\models\\research>protoc object_detection\\protos\\faster_rcnn_box_coder.proto --python_out=. \n",
      "\n",
      "(tf) d:\\TITO\\Documents\\Deep-learning\\eco-bag-detection\\models\\research>protoc object_detection\\protos\\flexible_grid_anchor_generator.proto --python_out=. \n",
      "\n",
      "(tf) d:\\TITO\\Documents\\Deep-learning\\eco-bag-detection\\models\\research>protoc object_detection\\protos\\fpn.proto --python_out=. \n",
      "\n",
      "(tf) d:\\TITO\\Documents\\Deep-learning\\eco-bag-detection\\models\\research>protoc object_detection\\protos\\graph_rewriter.proto --python_out=. \n",
      "\n",
      "(tf) d:\\TITO\\Documents\\Deep-learning\\eco-bag-detection\\models\\research>protoc object_detection\\protos\\grid_anchor_generator.proto --python_out=. \n",
      "\n",
      "(tf) d:\\TITO\\Documents\\Deep-learning\\eco-bag-detection\\models\\research>protoc object_detection\\protos\\hyperparams.proto --python_out=. \n",
      "\n",
      "(tf) d:\\TITO\\Documents\\Deep-learning\\eco-bag-detection\\models\\research>protoc object_detection\\protos\\image_resizer.proto --python_out=. \n",
      "\n",
      "(tf) d:\\TITO\\Documents\\Deep-learning\\eco-bag-detection\\models\\research>protoc object_detection\\protos\\input_reader.proto --python_out=. \n",
      "\n",
      "(tf) d:\\TITO\\Documents\\Deep-learning\\eco-bag-detection\\models\\research>protoc object_detection\\protos\\keypoint_box_coder.proto --python_out=. \n",
      "\n",
      "(tf) d:\\TITO\\Documents\\Deep-learning\\eco-bag-detection\\models\\research>protoc object_detection\\protos\\losses.proto --python_out=. \n",
      "\n",
      "(tf) d:\\TITO\\Documents\\Deep-learning\\eco-bag-detection\\models\\research>protoc object_detection\\protos\\matcher.proto --python_out=. \n",
      "\n",
      "(tf) d:\\TITO\\Documents\\Deep-learning\\eco-bag-detection\\models\\research>protoc object_detection\\protos\\mean_stddev_box_coder.proto --python_out=. \n",
      "\n",
      "(tf) d:\\TITO\\Documents\\Deep-learning\\eco-bag-detection\\models\\research>protoc object_detection\\protos\\model.proto --python_out=. \n",
      "\n",
      "(tf) d:\\TITO\\Documents\\Deep-learning\\eco-bag-detection\\models\\research>protoc object_detection\\protos\\multiscale_anchor_generator.proto --python_out=. \n",
      "\n",
      "(tf) d:\\TITO\\Documents\\Deep-learning\\eco-bag-detection\\models\\research>protoc object_detection\\protos\\optimizer.proto --python_out=. \n",
      "\n",
      "(tf) d:\\TITO\\Documents\\Deep-learning\\eco-bag-detection\\models\\research>protoc object_detection\\protos\\pipeline.proto --python_out=. \n",
      "\n",
      "(tf) d:\\TITO\\Documents\\Deep-learning\\eco-bag-detection\\models\\research>protoc object_detection\\protos\\post_processing.proto --python_out=. \n",
      "\n",
      "(tf) d:\\TITO\\Documents\\Deep-learning\\eco-bag-detection\\models\\research>protoc object_detection\\protos\\preprocessor.proto --python_out=. \n",
      "\n",
      "(tf) d:\\TITO\\Documents\\Deep-learning\\eco-bag-detection\\models\\research>protoc object_detection\\protos\\region_similarity_calculator.proto --python_out=. \n",
      "\n",
      "(tf) d:\\TITO\\Documents\\Deep-learning\\eco-bag-detection\\models\\research>protoc object_detection\\protos\\square_box_coder.proto --python_out=. \n",
      "\n",
      "(tf) d:\\TITO\\Documents\\Deep-learning\\eco-bag-detection\\models\\research>protoc object_detection\\protos\\ssd.proto --python_out=. \n",
      "\n",
      "(tf) d:\\TITO\\Documents\\Deep-learning\\eco-bag-detection\\models\\research>protoc object_detection\\protos\\ssd_anchor_generator.proto --python_out=. \n",
      "\n",
      "(tf) d:\\TITO\\Documents\\Deep-learning\\eco-bag-detection\\models\\research>protoc object_detection\\protos\\string_int_label_map.proto --python_out=. \n",
      "\n",
      "(tf) d:\\TITO\\Documents\\Deep-learning\\eco-bag-detection\\models\\research>protoc object_detection\\protos\\target_assigner.proto --python_out=. \n",
      "\n",
      "(tf) d:\\TITO\\Documents\\Deep-learning\\eco-bag-detection\\models\\research>protoc object_detection\\protos\\train.proto --python_out=. \n"
     ]
    }
   ],
   "source": [
    "!for /f %i in ('dir /b object_detection\\protos\\*.proto') do protoc object_detection\\protos\\%i --python_out=."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "import io\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "from six import BytesIO\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display, Javascript\n",
    "from IPython.display import Image as IPyImage\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils  import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(path):\n",
    "    \n",
    "    img_data = tf.io.gfile.GFile(path, 'rb').read()\n",
    "    image = Image.open(BytesIO(img_data))\n",
    "    (im_width, im_height) = image.size\n",
    "    \n",
    "    return np.array(image.getdata()).reshape(\n",
    "        (im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "\n",
    "def plot_detections(image_np,\n",
    "                    boxes,\n",
    "                    classes,\n",
    "                    scores,\n",
    "                    category_index,\n",
    "                    figsize=(12, 16),\n",
    "                    image_name=None):\n",
    "    \n",
    "    image_np_with_annotations = image_np.copy()\n",
    "    \n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np_with_annotations,\n",
    "        boxes,\n",
    "        classes,\n",
    "        scores,\n",
    "        category_index,\n",
    "        use_normalized_coordinates=False,\n",
    "        min_score_thresh=0.8)\n",
    "    \n",
    "    if image_name:\n",
    "        plt.imsave(image_name, image_np_with_annotations)\n",
    "    \n",
    "    else:\n",
    "        plt.imshow(image_np_with_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "append image from  1845126ThinkstockPhotos-147041313-reusable-bag780x390_jpg.rf.761b59e5fbae9a2ecbc158455e2b727e.jpg\n",
      "append from  1845126ThinkstockPhotos-147041313-reusable-bag780x390_jpg.rf.761b59e5fbae9a2ecbc158455e2b727e.xml\n",
      "append from  1845126ThinkstockPhotos-147041313-reusable-bag780x390_jpg.rf.761b59e5fbae9a2ecbc158455e2b727e.xml\n",
      "append from  1845126ThinkstockPhotos-147041313-reusable-bag780x390_jpg.rf.761b59e5fbae9a2ecbc158455e2b727e.xml\n",
      "append from  1845126ThinkstockPhotos-147041313-reusable-bag780x390_jpg.rf.761b59e5fbae9a2ecbc158455e2b727e.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  1845126ThinkstockPhotos-147041313-reusable-bag780x390_jpg.rf.d3d4aa054dc31f06f31a9f9bc8c9af24.jpg\n",
      "append from  1845126ThinkstockPhotos-147041313-reusable-bag780x390_jpg.rf.d3d4aa054dc31f06f31a9f9bc8c9af24.xml\n",
      "append from  1845126ThinkstockPhotos-147041313-reusable-bag780x390_jpg.rf.d3d4aa054dc31f06f31a9f9bc8c9af24.xml\n",
      "append from  1845126ThinkstockPhotos-147041313-reusable-bag780x390_jpg.rf.d3d4aa054dc31f06f31a9f9bc8c9af24.xml\n",
      "append from  1845126ThinkstockPhotos-147041313-reusable-bag780x390_jpg.rf.d3d4aa054dc31f06f31a9f9bc8c9af24.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  1845126ThinkstockPhotos-147041313-reusable-bag780x390_jpg.rf.f3657d8f25573067a2a8b79b84cbc797.jpg\n",
      "append from  1845126ThinkstockPhotos-147041313-reusable-bag780x390_jpg.rf.f3657d8f25573067a2a8b79b84cbc797.xml\n",
      "append from  1845126ThinkstockPhotos-147041313-reusable-bag780x390_jpg.rf.f3657d8f25573067a2a8b79b84cbc797.xml\n",
      "append from  1845126ThinkstockPhotos-147041313-reusable-bag780x390_jpg.rf.f3657d8f25573067a2a8b79b84cbc797.xml\n",
      "append from  1845126ThinkstockPhotos-147041313-reusable-bag780x390_jpg.rf.f3657d8f25573067a2a8b79b84cbc797.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  2--Triple-F-300x300_jpg.rf.2d9a58d9ab737972a447a99df744ecf2.jpg\n",
      "append from  2--Triple-F-300x300_jpg.rf.2d9a58d9ab737972a447a99df744ecf2.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  2--Triple-F-300x300_jpg.rf.c70a2539e65c5efd86f0c50077fc0e10.jpg\n",
      "append from  2--Triple-F-300x300_jpg.rf.c70a2539e65c5efd86f0c50077fc0e10.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  2--Triple-F-300x300_jpg.rf.fb924d56fa19db3ff552ced431135c28.jpg\n",
      "append from  2--Triple-F-300x300_jpg.rf.fb924d56fa19db3ff552ced431135c28.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  2000004931827_jpg.rf.15e38c1b23ae05985c3e0dd0f28d5b30.jpg\n",
      "append from  2000004931827_jpg.rf.15e38c1b23ae05985c3e0dd0f28d5b30.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  2000004931827_jpg.rf.29c35d043d269d6436283ec2d57431f1.jpg\n",
      "append from  2000004931827_jpg.rf.29c35d043d269d6436283ec2d57431f1.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  2000004931827_jpg.rf.b8002411ad698ad4d090566208a58aad.jpg\n",
      "append from  2000004931827_jpg.rf.b8002411ad698ad4d090566208a58aad.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  51XBkXVxTTL-_AC_UX385__jpg.rf.5f64e4ffcfd76aee865ec241ffc968f8.jpg\n",
      "append from  51XBkXVxTTL-_AC_UX385__jpg.rf.5f64e4ffcfd76aee865ec241ffc968f8.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  51XBkXVxTTL-_AC_UX385__jpg.rf.89a40331385e06ff656de18d41d5036f.jpg\n",
      "append from  51XBkXVxTTL-_AC_UX385__jpg.rf.89a40331385e06ff656de18d41d5036f.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  51XBkXVxTTL-_AC_UX385__jpg.rf.dcd4fd7b5ac4eae9504e0c5bc1471a11.jpg\n",
      "append from  51XBkXVxTTL-_AC_UX385__jpg.rf.dcd4fd7b5ac4eae9504e0c5bc1471a11.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  7076581_06664b72-29fe-439b-bbf5-7dbfcb2089dc_1080_1080_jpg.rf.0a3d0503498dc105cc533af0bc8c0f87.jpg\n",
      "append from  7076581_06664b72-29fe-439b-bbf5-7dbfcb2089dc_1080_1080_jpg.rf.0a3d0503498dc105cc533af0bc8c0f87.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  7076581_06664b72-29fe-439b-bbf5-7dbfcb2089dc_1080_1080_jpg.rf.2eba716b7646aeb2ed95ab878cc00899.jpg\n",
      "append from  7076581_06664b72-29fe-439b-bbf5-7dbfcb2089dc_1080_1080_jpg.rf.2eba716b7646aeb2ed95ab878cc00899.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  7076581_06664b72-29fe-439b-bbf5-7dbfcb2089dc_1080_1080_jpg.rf.dd3bc1dc1ba69ba76d8f66b075b0c579.jpg\n",
      "append from  7076581_06664b72-29fe-439b-bbf5-7dbfcb2089dc_1080_1080_jpg.rf.dd3bc1dc1ba69ba76d8f66b075b0c579.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  8301aed5cc1b471673087c3da0095e26_jpg.rf.25d4c2c538377e731f75c3c3434c8ef1.jpg\n",
      "append from  8301aed5cc1b471673087c3da0095e26_jpg.rf.25d4c2c538377e731f75c3c3434c8ef1.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  8301aed5cc1b471673087c3da0095e26_jpg.rf.ab1740ceff2d132601b580be215f12bc.jpg\n",
      "append from  8301aed5cc1b471673087c3da0095e26_jpg.rf.ab1740ceff2d132601b580be215f12bc.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  8301aed5cc1b471673087c3da0095e26_jpg.rf.b33204708e4d5a160bd10d8f58e850d7.jpg\n",
      "append from  8301aed5cc1b471673087c3da0095e26_jpg.rf.b33204708e4d5a160bd10d8f58e850d7.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  bb26326c-9813-4efc-a8dd-244c220a9ba6_jpg.rf.481b0276c26615f1b52d69bdbbbb9ba9.jpg\n",
      "append from  bb26326c-9813-4efc-a8dd-244c220a9ba6_jpg.rf.481b0276c26615f1b52d69bdbbbb9ba9.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  bb26326c-9813-4efc-a8dd-244c220a9ba6_jpg.rf.9ff5ea0b2b5e0f23a4f3cb67b9c72e66.jpg\n",
      "append from  bb26326c-9813-4efc-a8dd-244c220a9ba6_jpg.rf.9ff5ea0b2b5e0f23a4f3cb67b9c72e66.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  bb26326c-9813-4efc-a8dd-244c220a9ba6_jpg.rf.cb01b622394ebfd01c14a10126fa3cae.jpg\n",
      "append from  bb26326c-9813-4efc-a8dd-244c220a9ba6_jpg.rf.cb01b622394ebfd01c14a10126fa3cae.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  carry-bags-500x500_jpeg.rf.444de65cd8d2c9e36a46d4d7c0a66d97.jpg\n",
      "append from  carry-bags-500x500_jpeg.rf.444de65cd8d2c9e36a46d4d7c0a66d97.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  carry-bags-500x500_jpeg.rf.c5ae9e0e1a46b89d8de55f656e92cb2e.jpg\n",
      "append from  carry-bags-500x500_jpeg.rf.c5ae9e0e1a46b89d8de55f656e92cb2e.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  carry-bags-500x500_jpeg.rf.ce492f8eb272cb91178ed4ae43349687.jpg\n",
      "append from  carry-bags-500x500_jpeg.rf.ce492f8eb272cb91178ed4ae43349687.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  cfb1d874-4ed4-45c2-a488-3e289dbc4c70_jpg.rf.5bfeebc6df8470e66f992fb4559d4616.jpg\n",
      "append from  cfb1d874-4ed4-45c2-a488-3e289dbc4c70_jpg.rf.5bfeebc6df8470e66f992fb4559d4616.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  cfb1d874-4ed4-45c2-a488-3e289dbc4c70_jpg.rf.d6d9f9b8337c59b091985d198206d65d.jpg\n",
      "append from  cfb1d874-4ed4-45c2-a488-3e289dbc4c70_jpg.rf.d6d9f9b8337c59b091985d198206d65d.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  cfb1d874-4ed4-45c2-a488-3e289dbc4c70_jpg.rf.fd0e09c2d3669e03f08bb8c057518eef.jpg\n",
      "append from  cfb1d874-4ed4-45c2-a488-3e289dbc4c70_jpg.rf.fd0e09c2d3669e03f08bb8c057518eef.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  Cloth-Bag_jpg.rf.562158d1ba161ae4a056f6a1db081e7a.jpg\n",
      "append from  Cloth-Bag_jpg.rf.562158d1ba161ae4a056f6a1db081e7a.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  Cloth-Bag_jpg.rf.8075cade3197f3a01c0bf8f77aa086df.jpg\n",
      "append from  Cloth-Bag_jpg.rf.8075cade3197f3a01c0bf8f77aa086df.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  Cloth-Bag_jpg.rf.b206cfdc38e7f6b5318755fe7d044ae8.jpg\n",
      "append from  Cloth-Bag_jpg.rf.b206cfdc38e7f6b5318755fe7d044ae8.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  download--1-_jpg.rf.11ec6ce7fdc99823bc1e1d50c045afec.jpg\n",
      "append from  download--1-_jpg.rf.11ec6ce7fdc99823bc1e1d50c045afec.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  download--1-_jpg.rf.4cefee110adc1a304933cf4bcfe22a0b.jpg\n",
      "append from  download--1-_jpg.rf.4cefee110adc1a304933cf4bcfe22a0b.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  download--1-_jpg.rf.5ebbf10b725a22976fc6d30be1e91750.jpg\n",
      "append from  download--1-_jpg.rf.5ebbf10b725a22976fc6d30be1e91750.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  download--3-_jpg.rf.533a2ca5bb8a1d9b74c4b25177d39e31.jpg\n",
      "append from  download--3-_jpg.rf.533a2ca5bb8a1d9b74c4b25177d39e31.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  download--3-_jpg.rf.fbd7469fee75c72be6803e4f243f5a51.jpg\n",
      "append from  download--3-_jpg.rf.fbd7469fee75c72be6803e4f243f5a51.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  download--3-_jpg.rf.fc042ecb8d2501ec7c65c0782eddd135.jpg\n",
      "append from  download--3-_jpg.rf.fc042ecb8d2501ec7c65c0782eddd135.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  e011dfe9-e52c-412a-b598-663bd0d95455_jpg.rf.18ed57f693180031326b5afdb4362550.jpg\n",
      "append from  e011dfe9-e52c-412a-b598-663bd0d95455_jpg.rf.18ed57f693180031326b5afdb4362550.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  e011dfe9-e52c-412a-b598-663bd0d95455_jpg.rf.355e23451de8d722ca826562a6d2b912.jpg\n",
      "append from  e011dfe9-e52c-412a-b598-663bd0d95455_jpg.rf.355e23451de8d722ca826562a6d2b912.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  e011dfe9-e52c-412a-b598-663bd0d95455_jpg.rf.41d32934146e8c15477b581b2d046bab.jpg\n",
      "append from  e011dfe9-e52c-412a-b598-663bd0d95455_jpg.rf.41d32934146e8c15477b581b2d046bab.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  eco-friendly-cloth-bag-500x500_jpg.rf.bb602418c409ee67da2174b2dba58ad7.jpg\n",
      "append from  eco-friendly-cloth-bag-500x500_jpg.rf.bb602418c409ee67da2174b2dba58ad7.xml\n",
      "append from  eco-friendly-cloth-bag-500x500_jpg.rf.bb602418c409ee67da2174b2dba58ad7.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  eco-friendly-cloth-bag-500x500_jpg.rf.d92a4d9f4e4918a1034700802ed54b19.jpg\n",
      "append from  eco-friendly-cloth-bag-500x500_jpg.rf.d92a4d9f4e4918a1034700802ed54b19.xml\n",
      "append from  eco-friendly-cloth-bag-500x500_jpg.rf.d92a4d9f4e4918a1034700802ed54b19.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  eco-friendly-cloth-bag-500x500_jpg.rf.faf5d7e3c674d1c4a9891f2ddeeb93ff.jpg\n",
      "append from  eco-friendly-cloth-bag-500x500_jpg.rf.faf5d7e3c674d1c4a9891f2ddeeb93ff.xml\n",
      "append from  eco-friendly-cloth-bag-500x500_jpg.rf.faf5d7e3c674d1c4a9891f2ddeeb93ff.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  f2c3d55e4c42ccfec373ac6a780f761b_jpg.rf.a92685fb498a4577905fb0fa43293837.jpg\n",
      "append from  f2c3d55e4c42ccfec373ac6a780f761b_jpg.rf.a92685fb498a4577905fb0fa43293837.xml\n",
      "append from  f2c3d55e4c42ccfec373ac6a780f761b_jpg.rf.a92685fb498a4577905fb0fa43293837.xml\n",
      "append from  f2c3d55e4c42ccfec373ac6a780f761b_jpg.rf.a92685fb498a4577905fb0fa43293837.xml\n",
      "append from  f2c3d55e4c42ccfec373ac6a780f761b_jpg.rf.a92685fb498a4577905fb0fa43293837.xml\n",
      "append from  f2c3d55e4c42ccfec373ac6a780f761b_jpg.rf.a92685fb498a4577905fb0fa43293837.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  f2c3d55e4c42ccfec373ac6a780f761b_jpg.rf.fbe05ebaf05c5aada871fb196e3c84f8.jpg\n",
      "append from  f2c3d55e4c42ccfec373ac6a780f761b_jpg.rf.fbe05ebaf05c5aada871fb196e3c84f8.xml\n",
      "append from  f2c3d55e4c42ccfec373ac6a780f761b_jpg.rf.fbe05ebaf05c5aada871fb196e3c84f8.xml\n",
      "append from  f2c3d55e4c42ccfec373ac6a780f761b_jpg.rf.fbe05ebaf05c5aada871fb196e3c84f8.xml\n",
      "append from  f2c3d55e4c42ccfec373ac6a780f761b_jpg.rf.fbe05ebaf05c5aada871fb196e3c84f8.xml\n",
      "append from  f2c3d55e4c42ccfec373ac6a780f761b_jpg.rf.fbe05ebaf05c5aada871fb196e3c84f8.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  images--2-_jpg.rf.a44b1aec9333ba4910f0d6de30a1a6b0.jpg\n",
      "append from  images--2-_jpg.rf.a44b1aec9333ba4910f0d6de30a1a6b0.xml\n",
      "append from  images--2-_jpg.rf.a44b1aec9333ba4910f0d6de30a1a6b0.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  images--2-_jpg.rf.f50dcfeba15972822f646854cda5b227.jpg\n",
      "append from  images--2-_jpg.rf.f50dcfeba15972822f646854cda5b227.xml\n",
      "append from  images--2-_jpg.rf.f50dcfeba15972822f646854cda5b227.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  images--2-_jpg.rf.f6e2a9201fb5d7b74cf185732261e8f5.jpg\n",
      "append from  images--2-_jpg.rf.f6e2a9201fb5d7b74cf185732261e8f5.xml\n",
      "append from  images--2-_jpg.rf.f6e2a9201fb5d7b74cf185732261e8f5.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  images--3-_jpg.rf.303c5d3743c2ea0eb3b5cc1a4723e05c.jpg\n",
      "append from  images--3-_jpg.rf.303c5d3743c2ea0eb3b5cc1a4723e05c.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  images--3-_jpg.rf.83fed214c9fe1be316fda9b72e2c5e73.jpg\n",
      "append from  images--3-_jpg.rf.83fed214c9fe1be316fda9b72e2c5e73.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  images--3-_jpg.rf.b8f092c812a5cbc95bc8d959123e5d9c.jpg\n",
      "append from  images--3-_jpg.rf.b8f092c812a5cbc95bc8d959123e5d9c.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  images--4-_jpg.rf.abe732bbd2fe6bc9439a6d2adefa0d6c.jpg\n",
      "append from  images--4-_jpg.rf.abe732bbd2fe6bc9439a6d2adefa0d6c.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  images--4-_jpg.rf.e0095d2288d59869c3a48ec930e65515.jpg\n",
      "append from  images--4-_jpg.rf.e0095d2288d59869c3a48ec930e65515.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  images--4-_jpg.rf.e9d1e69fd6cb04fda579472fe95d1b6c.jpg\n",
      "append from  images--4-_jpg.rf.e9d1e69fd6cb04fda579472fe95d1b6c.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  images--5-_jpg.rf.1f826e455adc47c56a7eb831f42b5a9d.jpg\n",
      "append from  images--5-_jpg.rf.1f826e455adc47c56a7eb831f42b5a9d.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  images--5-_jpg.rf.bca6e42ad3d2b805138eb8165977c15b.jpg\n",
      "append from  images--5-_jpg.rf.bca6e42ad3d2b805138eb8165977c15b.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  images--5-_jpg.rf.ef3e0bbb49dd7fa4ac01322cc351e8b1.jpg\n",
      "append from  images--5-_jpg.rf.ef3e0bbb49dd7fa4ac01322cc351e8b1.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  images--6-_jpg.rf.1fa34eb520862cc5b24c43f40a0a993b.jpg\n",
      "append from  images--6-_jpg.rf.1fa34eb520862cc5b24c43f40a0a993b.xml\n",
      "append from  images--6-_jpg.rf.1fa34eb520862cc5b24c43f40a0a993b.xml\n",
      "append from  images--6-_jpg.rf.1fa34eb520862cc5b24c43f40a0a993b.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  images--6-_jpg.rf.6dadc0801e2cf235e43a4e6337ac94a0.jpg\n",
      "append from  images--6-_jpg.rf.6dadc0801e2cf235e43a4e6337ac94a0.xml\n",
      "append from  images--6-_jpg.rf.6dadc0801e2cf235e43a4e6337ac94a0.xml\n",
      "append from  images--6-_jpg.rf.6dadc0801e2cf235e43a4e6337ac94a0.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  images--6-_jpg.rf.af624ee9dd75fafae5501f4c92b38a0d.jpg\n",
      "append from  images--6-_jpg.rf.af624ee9dd75fafae5501f4c92b38a0d.xml\n",
      "append from  images--6-_jpg.rf.af624ee9dd75fafae5501f4c92b38a0d.xml\n",
      "append from  images--6-_jpg.rf.af624ee9dd75fafae5501f4c92b38a0d.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  istockphoto-1286396838-170667a_jpg.rf.209bc6e16d510b1a11bf5603394d6053.jpg\n",
      "append from  istockphoto-1286396838-170667a_jpg.rf.209bc6e16d510b1a11bf5603394d6053.xml\n",
      "append from  istockphoto-1286396838-170667a_jpg.rf.209bc6e16d510b1a11bf5603394d6053.xml\n",
      "append from  istockphoto-1286396838-170667a_jpg.rf.209bc6e16d510b1a11bf5603394d6053.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  istockphoto-1286396838-170667a_jpg.rf.bea0c733e54f161da95e6d9b95e8b7f4.jpg\n",
      "append from  istockphoto-1286396838-170667a_jpg.rf.bea0c733e54f161da95e6d9b95e8b7f4.xml\n",
      "append from  istockphoto-1286396838-170667a_jpg.rf.bea0c733e54f161da95e6d9b95e8b7f4.xml\n",
      "append from  istockphoto-1286396838-170667a_jpg.rf.bea0c733e54f161da95e6d9b95e8b7f4.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  istockphoto-1286396838-170667a_jpg.rf.c7bb1583d23e41d1588cfb29bc6a0e1b.jpg\n",
      "append from  istockphoto-1286396838-170667a_jpg.rf.c7bb1583d23e41d1588cfb29bc6a0e1b.xml\n",
      "append from  istockphoto-1286396838-170667a_jpg.rf.c7bb1583d23e41d1588cfb29bc6a0e1b.xml\n",
      "append from  istockphoto-1286396838-170667a_jpg.rf.c7bb1583d23e41d1588cfb29bc6a0e1b.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  pemprov-dki-gencarkan-sosialisasi-penggunaan-kantong-belanja-ramah-lingkungan-K6btvw3r8u_jpg.rf.00b2b744a11941ca518db7ae02c9c171.jpg\n",
      "append from  pemprov-dki-gencarkan-sosialisasi-penggunaan-kantong-belanja-ramah-lingkungan-K6btvw3r8u_jpg.rf.00b2b744a11941ca518db7ae02c9c171.xml\n",
      "append from  pemprov-dki-gencarkan-sosialisasi-penggunaan-kantong-belanja-ramah-lingkungan-K6btvw3r8u_jpg.rf.00b2b744a11941ca518db7ae02c9c171.xml\n",
      "append from  pemprov-dki-gencarkan-sosialisasi-penggunaan-kantong-belanja-ramah-lingkungan-K6btvw3r8u_jpg.rf.00b2b744a11941ca518db7ae02c9c171.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  pemprov-dki-gencarkan-sosialisasi-penggunaan-kantong-belanja-ramah-lingkungan-K6btvw3r8u_jpg.rf.497860594a86e672a60c6a43401cc1c8.jpg\n",
      "append from  pemprov-dki-gencarkan-sosialisasi-penggunaan-kantong-belanja-ramah-lingkungan-K6btvw3r8u_jpg.rf.497860594a86e672a60c6a43401cc1c8.xml\n",
      "append from  pemprov-dki-gencarkan-sosialisasi-penggunaan-kantong-belanja-ramah-lingkungan-K6btvw3r8u_jpg.rf.497860594a86e672a60c6a43401cc1c8.xml\n",
      "append from  pemprov-dki-gencarkan-sosialisasi-penggunaan-kantong-belanja-ramah-lingkungan-K6btvw3r8u_jpg.rf.497860594a86e672a60c6a43401cc1c8.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  pemprov-dki-gencarkan-sosialisasi-penggunaan-kantong-belanja-ramah-lingkungan-K6btvw3r8u_jpg.rf.6b4fd398a421d20cd2af6a5336770954.jpg\n",
      "append from  pemprov-dki-gencarkan-sosialisasi-penggunaan-kantong-belanja-ramah-lingkungan-K6btvw3r8u_jpg.rf.6b4fd398a421d20cd2af6a5336770954.xml\n",
      "append from  pemprov-dki-gencarkan-sosialisasi-penggunaan-kantong-belanja-ramah-lingkungan-K6btvw3r8u_jpg.rf.6b4fd398a421d20cd2af6a5336770954.xml\n",
      "append from  pemprov-dki-gencarkan-sosialisasi-penggunaan-kantong-belanja-ramah-lingkungan-K6btvw3r8u_jpg.rf.6b4fd398a421d20cd2af6a5336770954.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  tas-rotterdam_jpg.rf.3287780aa5ece4bd49a0bc8afa068340.jpg\n",
      "append from  tas-rotterdam_jpg.rf.3287780aa5ece4bd49a0bc8afa068340.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  tas-rotterdam_jpg.rf.d0e37cb97740055b834c36796367397d.jpg\n",
      "append from  tas-rotterdam_jpg.rf.d0e37cb97740055b834c36796367397d.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  tas_kain_1630705972_2640f073_progressive_jpg.rf.566e63cddb321fe3bb81bcf3fb5ba414.jpg\n",
      "append from  tas_kain_1630705972_2640f073_progressive_jpg.rf.566e63cddb321fe3bb81bcf3fb5ba414.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  tas_kain_1630705972_2640f073_progressive_jpg.rf.8f9a701f66c784f52f29d09bcc9ba396.jpg\n",
      "append from  tas_kain_1630705972_2640f073_progressive_jpg.rf.8f9a701f66c784f52f29d09bcc9ba396.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  tas_kain_1630705972_2640f073_progressive_jpg.rf.d8e7d4f765ddb9ef130d53e1898ff774.jpg\n",
      "append from  tas_kain_1630705972_2640f073_progressive_jpg.rf.d8e7d4f765ddb9ef130d53e1898ff774.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  tas_kain_besar_1558251975_43a7e77f_jpg.rf.82edc97ad838a10b7e64ee90cc60c2d2.jpg\n",
      "append from  tas_kain_besar_1558251975_43a7e77f_jpg.rf.82edc97ad838a10b7e64ee90cc60c2d2.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  tas_kain_besar_1558251975_43a7e77f_jpg.rf.a68be63b1f7808bfce94d3e527e7553d.jpg\n",
      "append from  tas_kain_besar_1558251975_43a7e77f_jpg.rf.a68be63b1f7808bfce94d3e527e7553d.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  tas_kain_besar_1558251975_43a7e77f_jpg.rf.e44492d6694f97c5b14531610349c6ff.jpg\n",
      "append from  tas_kain_besar_1558251975_43a7e77f_jpg.rf.e44492d6694f97c5b14531610349c6ff.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  THEBATIK-TAS-KAIN-BATIK-SOUVENIR-SEMINAR-1B_jpg.rf.2f5c90dd191c5ffb1762fb442064f343.jpg\n",
      "append from  THEBATIK-TAS-KAIN-BATIK-SOUVENIR-SEMINAR-1B_jpg.rf.2f5c90dd191c5ffb1762fb442064f343.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  THEBATIK-TAS-KAIN-BATIK-SOUVENIR-SEMINAR-1B_jpg.rf.6f811eac49b6375ddd32c8f4e1a60e2b.jpg\n",
      "append from  THEBATIK-TAS-KAIN-BATIK-SOUVENIR-SEMINAR-1B_jpg.rf.6f811eac49b6375ddd32c8f4e1a60e2b.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  THEBATIK-TAS-KAIN-BATIK-SOUVENIR-SEMINAR-1B_jpg.rf.a6bd47d8f1b11848d645bec00953bf1e.jpg\n",
      "append from  THEBATIK-TAS-KAIN-BATIK-SOUVENIR-SEMINAR-1B_jpg.rf.a6bd47d8f1b11848d645bec00953bf1e.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  WhatsApp-Image-2021-12-14-at-19-35-39_jpeg.rf.99b1da3cf086d2a4c172eff87104d6ef.jpg\n",
      "append from  WhatsApp-Image-2021-12-14-at-19-35-39_jpeg.rf.99b1da3cf086d2a4c172eff87104d6ef.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  WhatsApp-Image-2021-12-14-at-19-35-39_jpeg.rf.cbe9380202f5149362efff553206258d.jpg\n",
      "append from  WhatsApp-Image-2021-12-14-at-19-35-39_jpeg.rf.cbe9380202f5149362efff553206258d.xml\n",
      "append the coordinates of the bounding boxes to the main list\n",
      "append image from  WhatsApp-Image-2021-12-14-at-19-35-39_jpeg.rf.e6f69d714eacbeeab21fa6b96710e725.jpg\n",
      "append from  WhatsApp-Image-2021-12-14-at-19-35-39_jpeg.rf.e6f69d714eacbeeab21fa6b96710e725.xml\n",
      "append the coordinates of the bounding boxes to the main list\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "train_image_dir = 'D:\\\\TITO\\\\Documents\\\\Deep-learning\\\\eco-bag-detection\\\\dataset\\\\train'\n",
    "\n",
    "train_images_np = []\n",
    "gt_boxes = []\n",
    "\n",
    "# Get the list of files in the train_image_dir directory\n",
    "file_list = os.listdir(train_image_dir)\n",
    "\n",
    "for i, file_name in enumerate(file_list):\n",
    "    image_path = os.path.join(train_image_dir, file_name)\n",
    "    if not file_name.endswith('.xml'):\n",
    "        # load images into numpy arrays and append to a list\n",
    "        train_images_np.append(load_image_into_numpy_array(image_path))\n",
    "        print(\"append image from \", file_name)\n",
    "    if file_name.endswith('.xml'):\n",
    "        xml_path = os.path.join(train_image_dir, file_name)\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        gt_box = []\n",
    "        \n",
    "        for obj in root.findall('object'):\n",
    "            bbox = obj.find('bndbox')\n",
    "            xmin = int(bbox.find('xmin').text)\n",
    "            ymin = int(bbox.find('ymin').text)\n",
    "            xmax = int(bbox.find('xmax').text)\n",
    "            ymax = int(bbox.find('ymax').text)\n",
    "            \n",
    "            gt_box.append(np.array([[xmin, ymin, xmax, ymax]]))\n",
    "            print(\"append from \", file_name)\n",
    "        \n",
    "        gt_boxes.append(gt_box)  # Append the list of bounding boxes to the main list\n",
    "        print(\"append the coordinates of the bounding boxes to the main list\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 1 1]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [3 1 2]\n",
      "  [1 0 0]\n",
      "  [2 0 1]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [1 1 1]\n",
      "  [1 1 1]\n",
      "  ...\n",
      "  [3 1 2]\n",
      "  [1 0 0]\n",
      "  [2 0 1]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [1 1 1]\n",
      "  [3 3 3]\n",
      "  ...\n",
      "  [3 1 2]\n",
      "  [1 0 0]\n",
      "  [2 0 1]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 3 0]\n",
      "  [0 2 0]\n",
      "  [0 0 7]\n",
      "  ...\n",
      "  [1 4 0]\n",
      "  [2 5 0]\n",
      "  [3 6 0]]\n",
      "\n",
      " [[0 3 0]\n",
      "  [0 2 0]\n",
      "  [0 0 7]\n",
      "  ...\n",
      "  [0 2 1]\n",
      "  [2 4 3]\n",
      "  [4 6 5]]\n",
      "\n",
      " [[0 3 0]\n",
      "  [0 2 0]\n",
      "  [0 0 7]\n",
      "  ...\n",
      "  [0 0 5]\n",
      "  [0 0 5]\n",
      "  [0 0 5]]]\n",
      "[array([[  1,  58, 120, 334]]), array([[199,  55, 416, 416]]), array([[145,  89, 402, 416]]), array([[  1, 105, 200, 416]])]\n"
     ]
    }
   ],
   "source": [
    "print(train_images_np[0])\n",
    "print(gt_boxes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "eco_bag_id = 1\n",
    "\n",
    "category_index = {eco_bag_id: {\n",
    "    \"id\": eco_bag_id,\n",
    "    \"name\": \"Cloth-Bag\"\n",
    "}}\n",
    "\n",
    "num_classes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1, 'name': 'Cloth-Bag'}\n"
     ]
    }
   ],
   "source": [
    "print(category_index[eco_bag_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done prepping data.\n"
     ]
    }
   ],
   "source": [
    "label_id_offset = 1\n",
    "train_image_tensors = []\n",
    "\n",
    "gt_classes_one_hot_tensors = []\n",
    "gt_box_tensors = []\n",
    "\n",
    "for (train_image_np, gt_box_np) in zip(train_images_np, gt_boxes):\n",
    "    \n",
    "    train_image_tensors.append(tf.expand_dims(tf.convert_to_tensor(\n",
    "        train_image_np, dtype=tf.float32), axis=0))\n",
    "    \n",
    "    gt_box_np = np.array(gt_box_np)\n",
    "    gt_box_tensors.append(tf.convert_to_tensor(gt_box_np, dtype=tf.float32))\n",
    "    \n",
    "    zero_indexed_groundtruth_classes = tf.convert_to_tensor(\n",
    "        np.ones(shape=[gt_box_np.shape[0]], dtype=np.int32) - label_id_offset)\n",
    "    \n",
    "    # do one-hot encoding to ground truth classes\n",
    "    gt_classes_one_hot_tensors.append(tf.one_hot(\n",
    "        zero_indexed_groundtruth_classes, num_classes))\n",
    "\n",
    "print('Done prepping data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 13\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(gt_boxes)):\n\u001b[0;32m      9\u001b[0m     plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     10\u001b[0m     plot_detections(\n\u001b[0;32m     11\u001b[0m       train_images_np[idx],\n\u001b[0;32m     12\u001b[0m       gt_boxes[idx],\n\u001b[1;32m---> 13\u001b[0m       np\u001b[38;5;241m.\u001b[39mones(shape\u001b[38;5;241m=\u001b[39m[\u001b[43mgt_boxes\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32),\n\u001b[0;32m     14\u001b[0m       dummy_scores, category_index)\n\u001b[0;32m     16\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3000x1500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# give boxes a score of 100%\n",
    "dummy_scores = np.array([1.0], dtype=np.float32)\n",
    "\n",
    "# define the figure size\n",
    "plt.figure(figsize=(30, 15))\n",
    "\n",
    "# use the `plot_detections()` utility function to draw the ground truth boxes\n",
    "for idx in range(len(gt_boxes)):\n",
    "    plt.subplot(2, 4, idx+1)\n",
    "    plot_detections(\n",
    "      train_images_np[idx],\n",
    "      gt_boxes[idx],\n",
    "      np.ones(shape=[gt_boxes[idx].shape[0]], dtype=np.int32),\n",
    "      dummy_scores, category_index)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0  233M    0 19945    0     0  32555      0  2:05:20 --:--:--  2:05:20 32589\n",
      "  0  233M    0  705k    0     0   457k      0  0:08:42  0:00:01  0:08:41  458k\n",
      "  2  233M    2 4985k    0     0  1964k      0  0:02:01  0:00:02  0:01:59 1965k\n",
      "  3  233M    3 8501k    0     0  2402k      0  0:01:39  0:00:03  0:01:36 2402k\n",
      "  5  233M    5 11.7M    0     0  2645k      0  0:01:30  0:00:04  0:01:26 2646k\n",
      "  6  233M    6 15.1M    0     0  2802k      0  0:01:25  0:00:05  0:01:20 3146k\n",
      "  7  233M    7 18.5M    0     0  2909k      0  0:01:22  0:00:06  0:01:16 3665k\n",
      "  9  233M    9 22.0M    0     0  2989k      0  0:01:19  0:00:07  0:01:12 3509k\n",
      " 10  233M   10 25.4M    0     0  3049k      0  0:01:18  0:00:08  0:01:10 3506k\n",
      " 12  233M   12 28.8M    0     0  3099k      0  0:01:17  0:00:09  0:01:08 3511k\n",
      " 13  233M   13 32.2M    0     0  3138k      0  0:01:16  0:00:10  0:01:06 3511k\n",
      " 15  233M   15 35.7M    0     0  3171k      0  0:01:15  0:00:11  0:01:04 3512k\n",
      " 16  233M   16 39.1M    0     0  3198k      0  0:01:14  0:00:12  0:01:02 3512k\n",
      " 18  233M   18 42.5M    0     0  3221k      0  0:01:14  0:00:13  0:01:01 3516k\n",
      " 19  233M   19 46.0M    0     0  3241k      0  0:01:13  0:00:14  0:00:59 3512k\n",
      " 21  233M   21 49.4M    0     0  3259k      0  0:01:13  0:00:15  0:00:58 3513k\n",
      " 22  233M   22 52.8M    0     0  3274k      0  0:01:13  0:00:16  0:00:57 3512k\n",
      " 24  233M   24 56.3M    0     0  3287k      0  0:01:12  0:00:17  0:00:55 3513k\n",
      " 25  233M   25 59.7M    0     0  3300k      0  0:01:12  0:00:18  0:00:54 3513k\n",
      " 27  233M   27 63.1M    0     0  3311k      0  0:01:12  0:00:19  0:00:53 3513k\n",
      " 28  233M   28 66.5M    0     0  3320k      0  0:01:11  0:00:20  0:00:51 3511k\n",
      " 29  233M   29 70.0M    0     0  3329k      0  0:01:11  0:00:21  0:00:50 3511k\n",
      " 31  233M   31 73.4M    0     0  3337k      0  0:01:11  0:00:22  0:00:49 3512k\n",
      " 32  233M   32 76.8M    0     0  3345k      0  0:01:11  0:00:23  0:00:48 3511k\n",
      " 34  233M   34 80.2M    0     0  3349k      0  0:01:11  0:00:24  0:00:47 3497k\n",
      " 35  233M   35 83.6M    0     0  3355k      0  0:01:11  0:00:25  0:00:46 3497k\n",
      " 37  233M   37 87.1M    0     0  3361k      0  0:01:11  0:00:26  0:00:45 3498k\n",
      " 38  233M   38 90.5M    0     0  3365k      0  0:01:11  0:00:27  0:00:44 3490k\n",
      " 40  233M   40 93.9M    0     0  3370k      0  0:01:10  0:00:28  0:00:42 3490k\n",
      " 41  233M   41 97.0M    0     0  3364k      0  0:01:11  0:00:29  0:00:42 3438k\n",
      " 43  233M   43  100M    0     0  3369k      0  0:01:10  0:00:30  0:00:40 3439k\n",
      " 44  233M   44  103M    0     0  3373k      0  0:01:10  0:00:31  0:00:39 3439k\n",
      " 45  233M   45  107M    0     0  3377k      0  0:01:10  0:00:32  0:00:38 3446k\n",
      " 47  233M   47  110M    0     0  3381k      0  0:01:10  0:00:33  0:00:37 3446k\n",
      " 48  233M   48  113M    0     0  3378k      0  0:01:10  0:00:34  0:00:36 3460k\n",
      " 50  233M   50  117M    0     0  3389k      0  0:01:10  0:00:35  0:00:35 3512k\n",
      " 51  233M   51  121M    0     0  3392k      0  0:01:10  0:00:36  0:00:34 3512k\n",
      " 53  233M   53  124M    0     0  3395k      0  0:01:10  0:00:37  0:00:33 3506k\n",
      " 54  233M   54  127M    0     0  3398k      0  0:01:10  0:00:38  0:00:32 3507k\n",
      " 56  233M   56  131M    0     0  3394k      0  0:01:10  0:00:39  0:00:31 3507k\n",
      " 57  233M   57  134M    0     0  3403k      0  0:01:10  0:00:40  0:00:30 3506k\n",
      " 59  233M   59  138M    0     0  3406k      0  0:01:10  0:00:41  0:00:29 3505k\n",
      " 60  233M   60  141M    0     0  3408k      0  0:01:10  0:00:42  0:00:28 3512k\n",
      " 62  233M   62  145M    0     0  3411k      0  0:01:10  0:00:43  0:00:27 3511k\n",
      " 63  233M   63  148M    0     0  3413k      0  0:01:10  0:00:44  0:00:26 3562k\n",
      " 65  233M   65  151M    0     0  3415k      0  0:01:09  0:00:45  0:00:24 3511k\n",
      " 66  233M   66  155M    0     0  3417k      0  0:01:09  0:00:46  0:00:23 3512k\n",
      " 67  233M   67  156M    0     0  3369k      0  0:01:10  0:00:47  0:00:23 3039k\n",
      " 67  233M   67  157M    0     0  3318k      0  0:01:12  0:00:48  0:00:24 2512k\n",
      " 67  233M   67  158M    0     0  3269k      0  0:01:13  0:00:49  0:00:24 1985k\n",
      " 68  233M   68  159M    0     0  3221k      0  0:01:14  0:00:50  0:00:24 1456k\n",
      " 68  233M   68  159M    0     0  3176k      0  0:01:15  0:00:51  0:00:24  928k\n",
      " 69  233M   69  161M    0     0  3146k      0  0:01:15  0:00:52  0:00:23 1017k\n",
      " 70  233M   70  164M    0     0  3152k      0  0:01:15  0:00:53  0:00:22 1543k\n",
      " 72  233M   72  168M    0     0  3159k      0  0:01:15  0:00:54  0:00:21 2069k\n",
      " 73  233M   73  171M    0     0  3165k      0  0:01:15  0:00:55  0:00:20 2601k\n",
      " 73  233M   73  172M    0     0  3128k      0  0:01:16  0:00:56  0:00:20 2635k\n",
      " 75  233M   75  175M    0     0  3123k      0  0:01:16  0:00:57  0:00:19 2880k\n",
      " 76  233M   76  179M    0     0  3136k      0  0:01:16  0:00:58  0:00:18 2959k\n",
      " 77  233M   77  181M    0     0  3111k      0  0:01:16  0:00:59  0:00:17 2620k\n",
      " 78  233M   78  183M    0     0  3066k      0  0:01:17  0:01:01  0:00:16 2080k\n",
      " 78  233M   78  183M    0     0  3051k      0  0:01:18  0:01:01  0:00:17 2183k\n",
      " 79  233M   79  185M    0     0  3044k      0  0:01:18  0:01:02  0:00:16 2141k\n",
      " 80  233M   80  186M    0     0  3011k      0  0:01:19  0:01:03  0:00:16 1556k\n",
      " 81  233M   81  190M    0     0  3020k      0  0:01:19  0:01:04  0:00:15 1845k\n",
      " 82  233M   82  191M    0     0  2997k      0  0:01:19  0:01:05  0:00:14 2037k\n",
      " 83  233M   83  195M    0     0  3005k      0  0:01:19  0:01:06  0:00:13 2432k\n",
      " 85  233M   85  198M    0     0  3012k      0  0:01:19  0:01:07  0:00:12 2613k\n",
      " 86  233M   86  200M    0     0  3002k      0  0:01:19  0:01:08  0:00:11 2882k\n",
      " 86  233M   86  202M    0     0  2979k      0  0:01:20  0:01:09  0:00:11 2450k\n",
      " 88  233M   88  205M    0     0  2987k      0  0:01:20  0:01:10  0:00:10 2851k\n",
      " 89  233M   89  209M    0     0  2994k      0  0:01:19  0:01:11  0:00:08 2851k\n",
      " 91  233M   91  212M    0     0  3001k      0  0:01:19  0:01:12  0:00:07 2852k\n",
      " 91  233M   91  213M    0     0  2974k      0  0:01:20  0:01:13  0:00:07 2590k\n",
      " 92  233M   92  217M    0     0  2981k      0  0:01:20  0:01:14  0:00:06 3019k\n",
      " 94  233M   94  220M    0     0  2988k      0  0:01:19  0:01:15  0:00:04 3015k\n",
      " 95  233M   95  223M    0     0  2988k      0  0:01:20  0:01:16  0:00:04 2903k\n",
      " 97  233M   97  226M    0     0  2995k      0  0:01:19  0:01:17  0:00:02 2914k\n",
      " 98  233M   98  230M    0     0  3002k      0  0:01:19  0:01:18  0:00:01 3414k\n",
      "100  233M  100  233M    0     0  3008k      0  0:01:19  0:01:19 --:--:-- 3410k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0 dir(s) moved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    }
   ],
   "source": [
    "# !curl -O http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
    "    \n",
    "# # untar (decompress) the tar file\n",
    "# !tar -xf ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
    "\n",
    "# # copy the checkpoint to the test_data folder models/research/object_detection/test_data/\n",
    "# !move ssd_resnet50_v1_fpn_640x640_coco17_tpu-8\\checkpoint models\\research\\object_detection\\test_data\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': ssd {\n",
       "   num_classes: 90\n",
       "   image_resizer {\n",
       "     fixed_shape_resizer {\n",
       "       height: 640\n",
       "       width: 640\n",
       "     }\n",
       "   }\n",
       "   feature_extractor {\n",
       "     type: \"ssd_resnet50_v1_fpn_keras\"\n",
       "     depth_multiplier: 1.0\n",
       "     min_depth: 16\n",
       "     conv_hyperparams {\n",
       "       regularizer {\n",
       "         l2_regularizer {\n",
       "           weight: 0.00039999998989515007\n",
       "         }\n",
       "       }\n",
       "       initializer {\n",
       "         truncated_normal_initializer {\n",
       "           mean: 0.0\n",
       "           stddev: 0.029999999329447746\n",
       "         }\n",
       "       }\n",
       "       activation: RELU_6\n",
       "       batch_norm {\n",
       "         decay: 0.996999979019165\n",
       "         scale: true\n",
       "         epsilon: 0.0010000000474974513\n",
       "       }\n",
       "     }\n",
       "     override_base_feature_extractor_hyperparams: true\n",
       "     fpn {\n",
       "       min_level: 3\n",
       "       max_level: 7\n",
       "     }\n",
       "   }\n",
       "   box_coder {\n",
       "     faster_rcnn_box_coder {\n",
       "       y_scale: 10.0\n",
       "       x_scale: 10.0\n",
       "       height_scale: 5.0\n",
       "       width_scale: 5.0\n",
       "     }\n",
       "   }\n",
       "   matcher {\n",
       "     argmax_matcher {\n",
       "       matched_threshold: 0.5\n",
       "       unmatched_threshold: 0.5\n",
       "       ignore_thresholds: false\n",
       "       negatives_lower_than_unmatched: true\n",
       "       force_match_for_each_row: true\n",
       "       use_matmul_gather: true\n",
       "     }\n",
       "   }\n",
       "   similarity_calculator {\n",
       "     iou_similarity {\n",
       "     }\n",
       "   }\n",
       "   box_predictor {\n",
       "     weight_shared_convolutional_box_predictor {\n",
       "       conv_hyperparams {\n",
       "         regularizer {\n",
       "           l2_regularizer {\n",
       "             weight: 0.00039999998989515007\n",
       "           }\n",
       "         }\n",
       "         initializer {\n",
       "           random_normal_initializer {\n",
       "             mean: 0.0\n",
       "             stddev: 0.009999999776482582\n",
       "           }\n",
       "         }\n",
       "         activation: RELU_6\n",
       "         batch_norm {\n",
       "           decay: 0.996999979019165\n",
       "           scale: true\n",
       "           epsilon: 0.0010000000474974513\n",
       "         }\n",
       "       }\n",
       "       depth: 256\n",
       "       num_layers_before_predictor: 4\n",
       "       kernel_size: 3\n",
       "       class_prediction_bias_init: -4.599999904632568\n",
       "     }\n",
       "   }\n",
       "   anchor_generator {\n",
       "     multiscale_anchor_generator {\n",
       "       min_level: 3\n",
       "       max_level: 7\n",
       "       anchor_scale: 4.0\n",
       "       aspect_ratios: 1.0\n",
       "       aspect_ratios: 2.0\n",
       "       aspect_ratios: 0.5\n",
       "       scales_per_octave: 2\n",
       "     }\n",
       "   }\n",
       "   post_processing {\n",
       "     batch_non_max_suppression {\n",
       "       score_threshold: 9.99999993922529e-09\n",
       "       iou_threshold: 0.6000000238418579\n",
       "       max_detections_per_class: 100\n",
       "       max_total_detections: 100\n",
       "     }\n",
       "     score_converter: SIGMOID\n",
       "   }\n",
       "   normalize_loss_by_num_matches: true\n",
       "   loss {\n",
       "     localization_loss {\n",
       "       weighted_smooth_l1 {\n",
       "       }\n",
       "     }\n",
       "     classification_loss {\n",
       "       weighted_sigmoid_focal {\n",
       "         gamma: 2.0\n",
       "         alpha: 0.25\n",
       "       }\n",
       "     }\n",
       "     classification_weight: 1.0\n",
       "     localization_weight: 1.0\n",
       "   }\n",
       "   encode_background_as_zeros: true\n",
       "   normalize_loc_loss_by_codesize: true\n",
       "   inplace_batchnorm_update: true\n",
       "   freeze_batchnorm: false\n",
       " },\n",
       " 'train_config': batch_size: 64\n",
       " data_augmentation_options {\n",
       "   random_horizontal_flip {\n",
       "   }\n",
       " }\n",
       " data_augmentation_options {\n",
       "   random_crop_image {\n",
       "     min_object_covered: 0.0\n",
       "     min_aspect_ratio: 0.75\n",
       "     max_aspect_ratio: 3.0\n",
       "     min_area: 0.75\n",
       "     max_area: 1.0\n",
       "     overlap_thresh: 0.0\n",
       "   }\n",
       " }\n",
       " sync_replicas: true\n",
       " optimizer {\n",
       "   momentum_optimizer {\n",
       "     learning_rate {\n",
       "       cosine_decay_learning_rate {\n",
       "         learning_rate_base: 0.03999999910593033\n",
       "         total_steps: 25000\n",
       "         warmup_learning_rate: 0.013333000242710114\n",
       "         warmup_steps: 2000\n",
       "       }\n",
       "     }\n",
       "     momentum_optimizer_value: 0.8999999761581421\n",
       "   }\n",
       "   use_moving_average: false\n",
       " }\n",
       " fine_tune_checkpoint: \"PATH_TO_BE_CONFIGURED/resnet50.ckpt-1\"\n",
       " num_steps: 25000\n",
       " startup_delay_steps: 0.0\n",
       " replicas_to_aggregate: 8\n",
       " max_number_of_boxes: 100\n",
       " unpad_groundtruth_tensors: false\n",
       " fine_tune_checkpoint_type: \"classification\"\n",
       " use_bfloat16: true\n",
       " fine_tune_checkpoint_version: V2,\n",
       " 'train_input_config': label_map_path: \"PATH_TO_BE_CONFIGURED/label_map.txt\"\n",
       " tf_record_input_reader {\n",
       "   input_path: \"PATH_TO_BE_CONFIGURED/train2017-?????-of-00256.tfrecord\"\n",
       " },\n",
       " 'eval_config': metrics_set: \"coco_detection_metrics\"\n",
       " use_moving_averages: false,\n",
       " 'eval_input_configs': [label_map_path: \"PATH_TO_BE_CONFIGURED/label_map.txt\"\n",
       " shuffle: false\n",
       " num_epochs: 1\n",
       " tf_record_input_reader {\n",
       "   input_path: \"PATH_TO_BE_CONFIGURED/val2017-?????-of-00032.tfrecord\"\n",
       " }\n",
       " ],\n",
       " 'eval_input_config': label_map_path: \"PATH_TO_BE_CONFIGURED/label_map.txt\"\n",
       " shuffle: false\n",
       " num_epochs: 1\n",
       " tf_record_input_reader {\n",
       "   input_path: \"PATH_TO_BE_CONFIGURED/val2017-?????-of-00032.tfrecord\"\n",
       " }}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "### START CODE HERE ###\n",
    "# define the path to the .config file for ssd resnet 50 v1 640x640\n",
    "pipeline_config = \"./object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config\"\n",
    "\n",
    "# Load the configuration file into a dictionary\n",
    "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
    "\n",
    "### END CODE HERE ###\n",
    "# See what configs looks like\n",
    "configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ssd {\n",
       "  num_classes: 90\n",
       "  image_resizer {\n",
       "    fixed_shape_resizer {\n",
       "      height: 640\n",
       "      width: 640\n",
       "    }\n",
       "  }\n",
       "  feature_extractor {\n",
       "    type: \"ssd_resnet50_v1_fpn_keras\"\n",
       "    depth_multiplier: 1.0\n",
       "    min_depth: 16\n",
       "    conv_hyperparams {\n",
       "      regularizer {\n",
       "        l2_regularizer {\n",
       "          weight: 0.00039999998989515007\n",
       "        }\n",
       "      }\n",
       "      initializer {\n",
       "        truncated_normal_initializer {\n",
       "          mean: 0.0\n",
       "          stddev: 0.029999999329447746\n",
       "        }\n",
       "      }\n",
       "      activation: RELU_6\n",
       "      batch_norm {\n",
       "        decay: 0.996999979019165\n",
       "        scale: true\n",
       "        epsilon: 0.0010000000474974513\n",
       "      }\n",
       "    }\n",
       "    override_base_feature_extractor_hyperparams: true\n",
       "    fpn {\n",
       "      min_level: 3\n",
       "      max_level: 7\n",
       "    }\n",
       "  }\n",
       "  box_coder {\n",
       "    faster_rcnn_box_coder {\n",
       "      y_scale: 10.0\n",
       "      x_scale: 10.0\n",
       "      height_scale: 5.0\n",
       "      width_scale: 5.0\n",
       "    }\n",
       "  }\n",
       "  matcher {\n",
       "    argmax_matcher {\n",
       "      matched_threshold: 0.5\n",
       "      unmatched_threshold: 0.5\n",
       "      ignore_thresholds: false\n",
       "      negatives_lower_than_unmatched: true\n",
       "      force_match_for_each_row: true\n",
       "      use_matmul_gather: true\n",
       "    }\n",
       "  }\n",
       "  similarity_calculator {\n",
       "    iou_similarity {\n",
       "    }\n",
       "  }\n",
       "  box_predictor {\n",
       "    weight_shared_convolutional_box_predictor {\n",
       "      conv_hyperparams {\n",
       "        regularizer {\n",
       "          l2_regularizer {\n",
       "            weight: 0.00039999998989515007\n",
       "          }\n",
       "        }\n",
       "        initializer {\n",
       "          random_normal_initializer {\n",
       "            mean: 0.0\n",
       "            stddev: 0.009999999776482582\n",
       "          }\n",
       "        }\n",
       "        activation: RELU_6\n",
       "        batch_norm {\n",
       "          decay: 0.996999979019165\n",
       "          scale: true\n",
       "          epsilon: 0.0010000000474974513\n",
       "        }\n",
       "      }\n",
       "      depth: 256\n",
       "      num_layers_before_predictor: 4\n",
       "      kernel_size: 3\n",
       "      class_prediction_bias_init: -4.599999904632568\n",
       "    }\n",
       "  }\n",
       "  anchor_generator {\n",
       "    multiscale_anchor_generator {\n",
       "      min_level: 3\n",
       "      max_level: 7\n",
       "      anchor_scale: 4.0\n",
       "      aspect_ratios: 1.0\n",
       "      aspect_ratios: 2.0\n",
       "      aspect_ratios: 0.5\n",
       "      scales_per_octave: 2\n",
       "    }\n",
       "  }\n",
       "  post_processing {\n",
       "    batch_non_max_suppression {\n",
       "      score_threshold: 9.99999993922529e-09\n",
       "      iou_threshold: 0.6000000238418579\n",
       "      max_detections_per_class: 100\n",
       "      max_total_detections: 100\n",
       "    }\n",
       "    score_converter: SIGMOID\n",
       "  }\n",
       "  normalize_loss_by_num_matches: true\n",
       "  loss {\n",
       "    localization_loss {\n",
       "      weighted_smooth_l1 {\n",
       "      }\n",
       "    }\n",
       "    classification_loss {\n",
       "      weighted_sigmoid_focal {\n",
       "        gamma: 2.0\n",
       "        alpha: 0.25\n",
       "      }\n",
       "    }\n",
       "    classification_weight: 1.0\n",
       "    localization_weight: 1.0\n",
       "  }\n",
       "  encode_background_as_zeros: true\n",
       "  normalize_loc_loss_by_codesize: true\n",
       "  inplace_batchnorm_update: true\n",
       "  freeze_batchnorm: false\n",
       "}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "# Read in the object stored at the key 'model' of the configs dictionary\n",
    "model_config = configs[\"model\"]\n",
    "\n",
    "### END CODE HERE\n",
    "# see what model_config looks like\n",
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ssd {\n",
       "  num_classes: 1\n",
       "  image_resizer {\n",
       "    fixed_shape_resizer {\n",
       "      height: 640\n",
       "      width: 640\n",
       "    }\n",
       "  }\n",
       "  feature_extractor {\n",
       "    type: \"ssd_resnet50_v1_fpn_keras\"\n",
       "    depth_multiplier: 1.0\n",
       "    min_depth: 16\n",
       "    conv_hyperparams {\n",
       "      regularizer {\n",
       "        l2_regularizer {\n",
       "          weight: 0.00039999998989515007\n",
       "        }\n",
       "      }\n",
       "      initializer {\n",
       "        truncated_normal_initializer {\n",
       "          mean: 0.0\n",
       "          stddev: 0.029999999329447746\n",
       "        }\n",
       "      }\n",
       "      activation: RELU_6\n",
       "      batch_norm {\n",
       "        decay: 0.996999979019165\n",
       "        scale: true\n",
       "        epsilon: 0.0010000000474974513\n",
       "      }\n",
       "    }\n",
       "    override_base_feature_extractor_hyperparams: true\n",
       "    fpn {\n",
       "      min_level: 3\n",
       "      max_level: 7\n",
       "    }\n",
       "  }\n",
       "  box_coder {\n",
       "    faster_rcnn_box_coder {\n",
       "      y_scale: 10.0\n",
       "      x_scale: 10.0\n",
       "      height_scale: 5.0\n",
       "      width_scale: 5.0\n",
       "    }\n",
       "  }\n",
       "  matcher {\n",
       "    argmax_matcher {\n",
       "      matched_threshold: 0.5\n",
       "      unmatched_threshold: 0.5\n",
       "      ignore_thresholds: false\n",
       "      negatives_lower_than_unmatched: true\n",
       "      force_match_for_each_row: true\n",
       "      use_matmul_gather: true\n",
       "    }\n",
       "  }\n",
       "  similarity_calculator {\n",
       "    iou_similarity {\n",
       "    }\n",
       "  }\n",
       "  box_predictor {\n",
       "    weight_shared_convolutional_box_predictor {\n",
       "      conv_hyperparams {\n",
       "        regularizer {\n",
       "          l2_regularizer {\n",
       "            weight: 0.00039999998989515007\n",
       "          }\n",
       "        }\n",
       "        initializer {\n",
       "          random_normal_initializer {\n",
       "            mean: 0.0\n",
       "            stddev: 0.009999999776482582\n",
       "          }\n",
       "        }\n",
       "        activation: RELU_6\n",
       "        batch_norm {\n",
       "          decay: 0.996999979019165\n",
       "          scale: true\n",
       "          epsilon: 0.0010000000474974513\n",
       "        }\n",
       "      }\n",
       "      depth: 256\n",
       "      num_layers_before_predictor: 4\n",
       "      kernel_size: 3\n",
       "      class_prediction_bias_init: -4.599999904632568\n",
       "    }\n",
       "  }\n",
       "  anchor_generator {\n",
       "    multiscale_anchor_generator {\n",
       "      min_level: 3\n",
       "      max_level: 7\n",
       "      anchor_scale: 4.0\n",
       "      aspect_ratios: 1.0\n",
       "      aspect_ratios: 2.0\n",
       "      aspect_ratios: 0.5\n",
       "      scales_per_octave: 2\n",
       "    }\n",
       "  }\n",
       "  post_processing {\n",
       "    batch_non_max_suppression {\n",
       "      score_threshold: 9.99999993922529e-09\n",
       "      iou_threshold: 0.6000000238418579\n",
       "      max_detections_per_class: 100\n",
       "      max_total_detections: 100\n",
       "    }\n",
       "    score_converter: SIGMOID\n",
       "  }\n",
       "  normalize_loss_by_num_matches: true\n",
       "  loss {\n",
       "    localization_loss {\n",
       "      weighted_smooth_l1 {\n",
       "      }\n",
       "    }\n",
       "    classification_loss {\n",
       "      weighted_sigmoid_focal {\n",
       "        gamma: 2.0\n",
       "        alpha: 0.25\n",
       "      }\n",
       "    }\n",
       "    classification_weight: 1.0\n",
       "    localization_weight: 1.0\n",
       "  }\n",
       "  encode_background_as_zeros: true\n",
       "  normalize_loc_loss_by_codesize: true\n",
       "  inplace_batchnorm_update: true\n",
       "  freeze_batchnorm: true\n",
       "}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "# Modify the number of classes from its default of 90\n",
    "model_config.ssd.num_classes = num_classes\n",
    "\n",
    "# Freeze batch normalization\n",
    "model_config.ssd.freeze_batchnorm = True\n",
    "\n",
    "### END CODE HERE\n",
    "\n",
    "# See what model_config now looks like after you've customized it!\n",
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch'>\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE (Replace instances of `None` with your code) ###\n",
    "detection_model = model_builder.build(\n",
    "    model_config, is_training= True\n",
    ")\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(type(detection_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<object_detection.meta_architectures.ssd_meta_arch.SSDMetaArch at 0x21a40432550>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_self_setattr_tracking': True,\n",
       " '_obj_reference_counts_dict': ObjectIdentityDictionary({<_ObjectIdentityWrapper wrapping 1>: 1, <_ObjectIdentityWrapper wrapping DictWrapper({})>: 1, <_ObjectIdentityWrapper wrapping True>: 7, <_ObjectIdentityWrapper wrapping <object_detection.anchor_generators.multiscale_grid_anchor_generator.MultiscaleGridAnchorGenerator object at 0x0000021A3889B520>>: 1, <_ObjectIdentityWrapper wrapping <object_detection.predictors.convolutional_keras_box_predictor.WeightSharedConvolutionalBoxPredictor object at 0x0000021A41C90640>>: 1, <_ObjectIdentityWrapper wrapping <object_detection.box_coders.faster_rcnn_box_coder.FasterRcnnBoxCoder object at 0x0000021A384220A0>>: 1, <_ObjectIdentityWrapper wrapping <object_detection.models.ssd_resnet_v1_fpn_keras_feature_extractor.SSDResNet50V1FpnKerasFeatureExtractor object at 0x0000021A434401C0>>: 1, <_ObjectIdentityWrapper wrapping False>: 3, <_ObjectIdentityWrapper wrapping 'ResNet50V1_FPN'>: 1, <_ObjectIdentityWrapper wrapping <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>>: 1, <_ObjectIdentityWrapper wrapping <object_detection.core.target_assigner.TargetAssigner object at 0x0000021A404321C0>>: 1, <_ObjectIdentityWrapper wrapping <object_detection.core.losses.SigmoidFocalClassificationLoss object at 0x0000021A41C90BB0>>: 1, <_ObjectIdentityWrapper wrapping <object_detection.core.losses.WeightedSmoothL1LocalizationLoss object at 0x0000021A40432970>>: 1, <_ObjectIdentityWrapper wrapping 1.0>: 1, <_ObjectIdentityWrapper wrapping 1.0>: 1, <_ObjectIdentityWrapper wrapping 16>: 1, <_ObjectIdentityWrapper wrapping functools.partial(<function resize_image at 0x0000021A3605EF70>, new_height=640, new_width=640, method=0)>: 1, <_ObjectIdentityWrapper wrapping functools.partial(<function batch_multiclass_non_max_suppression at 0x0000021A371B03A0>, score_thresh=9.99999993922529e-09, iou_thresh=0.6000000238418579, max_size_per_class=100, max_total_size=100, use_static_shapes=False, use_class_agnostic_nms=False, max_classes_per_detection=1, soft_nms_sigma=0.0, use_partitioned_nms=False, use_combined_nms=False, change_coordinate_frame=True, use_hard_nms=False, use_cpu_nms=False)>: 1, <_ObjectIdentityWrapper wrapping <function _score_converter_fn_with_logit_scale.<locals>.score_converter_fn at 0x0000021A41E23310>>: 1, <_ObjectIdentityWrapper wrapping ListWrapper([])>: 1, <_ObjectIdentityWrapper wrapping 1.0>: 1, <_ObjectIdentityWrapper wrapping EqualizationLossConfig(weight=0.0, exclude_prefixes=[])>: 1}),\n",
       " '_num_classes': 1,\n",
       " '_self_unconditional_checkpoint_dependencies': [TrackableReference(name=_groundtruth_lists, ref={}),\n",
       "  TrackableReference(name=_box_predictor, ref=<object_detection.predictors.convolutional_keras_box_predictor.WeightSharedConvolutionalBoxPredictor object at 0x0000021A41C90640>),\n",
       "  TrackableReference(name=_feature_extractor, ref=<object_detection.models.ssd_resnet_v1_fpn_keras_feature_extractor.SSDResNet50V1FpnKerasFeatureExtractor object at 0x0000021A434401C0>),\n",
       "  TrackableReference(name=_batched_prediction_tensor_names, ref=ListWrapper([]))],\n",
       " '_self_unconditional_dependency_names': {'_groundtruth_lists': {},\n",
       "  '_box_predictor': <object_detection.predictors.convolutional_keras_box_predictor.WeightSharedConvolutionalBoxPredictor at 0x21a41c90640>,\n",
       "  '_feature_extractor': <object_detection.models.ssd_resnet_v1_fpn_keras_feature_extractor.SSDResNet50V1FpnKerasFeatureExtractor at 0x21a434401c0>,\n",
       "  '_batched_prediction_tensor_names': ListWrapper([])},\n",
       " '_self_unconditional_deferred_dependencies': {},\n",
       " '_self_update_uid': -1,\n",
       " '_self_name_based_restores': set(),\n",
       " '_self_saveable_object_factories': {},\n",
       " '_self_tracked_trackables': [{},\n",
       "  <object_detection.predictors.convolutional_keras_box_predictor.WeightSharedConvolutionalBoxPredictor at 0x21a41c90640>,\n",
       "  <object_detection.models.ssd_resnet_v1_fpn_keras_feature_extractor.SSDResNet50V1FpnKerasFeatureExtractor at 0x21a434401c0>,\n",
       "  ListWrapper([])],\n",
       " '_groundtruth_lists': {},\n",
       " '_training_step': None,\n",
       " '_instrumented_keras_api': True,\n",
       " '_instrumented_keras_layer_class': True,\n",
       " '_instrumented_keras_model_class': False,\n",
       " '_trainable': True,\n",
       " '_stateful': False,\n",
       " 'built': False,\n",
       " '_input_spec': None,\n",
       " '_build_input_shape': None,\n",
       " '_saved_model_inputs_spec': None,\n",
       " '_saved_model_arg_spec': None,\n",
       " '_supports_masking': False,\n",
       " '_name': 'ssd_meta_arch',\n",
       " '_activity_regularizer': None,\n",
       " '_trainable_weights': [],\n",
       " '_non_trainable_weights': [],\n",
       " '_updates': [],\n",
       " '_thread_local': <_thread._local at 0x21a41de9ae0>,\n",
       " '_callable_losses': [],\n",
       " '_losses': [],\n",
       " '_metrics': [],\n",
       " '_metrics_lock': <unlocked _thread.lock object at 0x0000021A40432840>,\n",
       " '_dtype_policy': <Policy \"float32\">,\n",
       " '_compute_dtype_object': tf.float32,\n",
       " '_autocast': True,\n",
       " '_inbound_nodes_value': [],\n",
       " '_outbound_nodes_value': [],\n",
       " '_call_spec': <keras.utils.layer_utils.CallFunctionSpec at 0x21a40432bb0>,\n",
       " '_dynamic': False,\n",
       " '_initial_weights': None,\n",
       " '_auto_track_sub_layers': True,\n",
       " '_preserve_input_structure_in_config': False,\n",
       " '_name_scope_on_declaration': '',\n",
       " '_captured_weight_regularizer': [],\n",
       " '_is_training': True,\n",
       " '_freeze_batchnorm': True,\n",
       " '_inplace_batchnorm_update': True,\n",
       " '_anchor_generator': <object_detection.anchor_generators.multiscale_grid_anchor_generator.MultiscaleGridAnchorGenerator at 0x21a3889b520>,\n",
       " '_box_predictor': <object_detection.predictors.convolutional_keras_box_predictor.WeightSharedConvolutionalBoxPredictor at 0x21a41c90640>,\n",
       " '_box_coder': <object_detection.box_coders.faster_rcnn_box_coder.FasterRcnnBoxCoder at 0x21a384220a0>,\n",
       " '_feature_extractor': <object_detection.models.ssd_resnet_v1_fpn_keras_feature_extractor.SSDResNet50V1FpnKerasFeatureExtractor at 0x21a434401c0>,\n",
       " '_add_background_class': True,\n",
       " '_explicit_background_class': False,\n",
       " '_extract_features_scope': 'ResNet50V1_FPN',\n",
       " '_unmatched_class_label': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>,\n",
       " '_target_assigner': <object_detection.core.target_assigner.TargetAssigner at 0x21a404321c0>,\n",
       " '_classification_loss': <object_detection.core.losses.SigmoidFocalClassificationLoss at 0x21a41c90bb0>,\n",
       " '_localization_loss': <object_detection.core.losses.WeightedSmoothL1LocalizationLoss at 0x21a40432970>,\n",
       " '_classification_loss_weight': 1.0,\n",
       " '_localization_loss_weight': 1.0,\n",
       " '_normalize_loss_by_num_matches': True,\n",
       " '_normalize_loc_loss_by_codesize': True,\n",
       " '_hard_example_miner': None,\n",
       " '_random_example_sampler': None,\n",
       " '_parallel_iterations': 16,\n",
       " '_image_resizer_fn': functools.partial(<function resize_image at 0x0000021A3605EF70>, new_height=640, new_width=640, method=0),\n",
       " '_non_max_suppression_fn': functools.partial(<function batch_multiclass_non_max_suppression at 0x0000021A371B03A0>, score_thresh=9.99999993922529e-09, iou_thresh=0.6000000238418579, max_size_per_class=100, max_total_size=100, use_static_shapes=False, use_class_agnostic_nms=False, max_classes_per_detection=1, soft_nms_sigma=0.0, use_partitioned_nms=False, use_combined_nms=False, change_coordinate_frame=True, use_hard_nms=False, use_cpu_nms=False),\n",
       " '_score_conversion_fn': <function object_detection.builders.post_processing_builder._score_converter_fn_with_logit_scale.<locals>.score_converter_fn(logits)>,\n",
       " '_anchors': None,\n",
       " '_add_summaries': True,\n",
       " '_batched_prediction_tensor_names': ListWrapper([]),\n",
       " '_expected_loss_weights_fn': None,\n",
       " '_use_confidences_as_targets': False,\n",
       " '_implicit_example_weight': 1.0,\n",
       " '_equalization_loss_config': EqualizationLossConfig(weight=0.0, exclude_prefixes=[]),\n",
       " '_return_raw_detections_during_predict': False}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(detection_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_box_predictor_checkpoint = tf.compat.v2.train.Checkpoint(\n",
    "    _base_tower_layers_for_heads=detection_model._box_predictor._base_tower_layers_for_heads,\n",
    "    # _prediction_heads=detection_model._box_predictor._prediction_heads,\n",
    "    #    (i.e., the classification head that we *will not* restore)\n",
    "    _box_prediction_head=detection_model._box_predictor._box_prediction_head,\n",
    "    )\n",
    "    \n",
    "    \n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.checkpoint.checkpoint.Checkpoint"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tmp_box_predictor_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_save_counter': None,\n",
       " '_save_assign_op': None,\n",
       " '_self_setattr_tracking': True,\n",
       " '_self_unconditional_checkpoint_dependencies': [TrackableReference(name=_base_tower_layers_for_heads, ref={'box_encodings': ListWrapper([]), 'class_predictions_with_background': ListWrapper([])}),\n",
       "  TrackableReference(name=_box_prediction_head, ref=<object_detection.predictors.heads.keras_box_head.WeightSharedConvolutionalBoxHead object at 0x0000021A3889BDF0>)],\n",
       " '_self_unconditional_dependency_names': {'_base_tower_layers_for_heads': {'box_encodings': ListWrapper([]),\n",
       "   'class_predictions_with_background': ListWrapper([])},\n",
       "  '_box_prediction_head': <object_detection.predictors.heads.keras_box_head.WeightSharedConvolutionalBoxHead at 0x21a3889bdf0>},\n",
       " '_self_unconditional_deferred_dependencies': {},\n",
       " '_self_update_uid': -1,\n",
       " '_self_name_based_restores': set(),\n",
       " '_self_saveable_object_factories': {},\n",
       " '_base_tower_layers_for_heads': {'box_encodings': ListWrapper([]),\n",
       "  'class_predictions_with_background': ListWrapper([])},\n",
       " '_box_prediction_head': <object_detection.predictors.heads.keras_box_head.WeightSharedConvolutionalBoxHead at 0x21a3889bdf0>,\n",
       " '_saver': <tensorflow.python.checkpoint.checkpoint.TrackableSaver at 0x21a38d58280>,\n",
       " '_attached_dependencies': None}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(tmp_box_predictor_checkpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
